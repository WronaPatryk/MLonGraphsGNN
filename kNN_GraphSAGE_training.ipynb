{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from source.utils import train_val_test_labels, labels_to_numbers, load_data_df, describe_pyg_data\n",
    "from source.structure_modeling import torch_geometric_data_from_graph\n",
    "from source.models import GraphSAGE, GraphSAGE2, train_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dir=\"audio/\"\n",
    "labels_train, labels_valid, labels_test = train_val_test_labels(audio_dir, \"training_list.txt\", \"validation_list.txt\", \"testing_list.txt\", index_col=None, header=None, pos=0)\n",
    "labels = labels_train + labels_valid + labels_test\n",
    "label_names, labels_nr = labels_to_numbers(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectr_dir = \"spectrograms/\"\n",
    "spectr_labels_train, spectr_labels_valid, spectr_labels_test = train_val_test_labels(spectr_dir, \"spectr_train_list.txt\", \"spectr_valid_list.txt\", \"spectr_test_list.txt\", index_col=0, header=0, pos=1)\n",
    "spectr_labels = spectr_labels_train + spectr_labels_valid + spectr_labels_test\n",
    "spectr_label_names, spectr_labels_nr = labels_to_numbers(spectr_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_audio_df = load_data_df(\"data/raw_audio/\", header=0)\n",
    "spectr2_df = load_data_df(\"data/spectrogram2/\")\n",
    "spectr3_df = load_data_df(\"data/spectrogram3/\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_audio_G = nx.read_adjlist(\"graphs/raw_audio_kNN.adjlist\")\n",
    "spectr2_G = nx.read_adjlist(\"graphs/spectr2_kNN.adjlist\")\n",
    "spectr3_G = nx.read_adjlist(\"graphs/spectr3_kNN.adjlist\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create torch_geometric.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 23682\n",
      "Number of edges: 185200\n",
      "Number of features: 1600\n",
      "Number of classes: 10\n",
      "Average node degree: 7.82\n",
      "Number of training nodes: 18538\n",
      "Number of validation nodes: 2567\n",
      "Number of testing nodes: 2577\n",
      "Training node label rate: 0.78\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "raw_audio_data = torch_geometric_data_from_graph(raw_audio_G, raw_audio_df, labels_nr, len(labels_train), len(labels_valid), len(labels_test))\n",
    "describe_pyg_data(raw_audio_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 23682\n",
      "Number of edges: 151290\n",
      "Number of features: 64\n",
      "Number of classes: 10\n",
      "Average node degree: 6.39\n",
      "Number of training nodes: 18538\n",
      "Number of validation nodes: 2567\n",
      "Number of testing nodes: 2577\n",
      "Training node label rate: 0.78\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "spectr2_data = torch_geometric_data_from_graph(spectr2_G, spectr2_df, spectr_labels_nr, len(spectr_labels_train), len(spectr_labels_valid), len(spectr_labels_test))\n",
    "describe_pyg_data(spectr2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 23682\n",
      "Number of edges: 155866\n",
      "Number of features: 256\n",
      "Number of classes: 10\n",
      "Average node degree: 6.58\n",
      "Number of training nodes: 18538\n",
      "Number of validation nodes: 2567\n",
      "Number of testing nodes: 2577\n",
      "Training node label rate: 0.78\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "spectr3_data = torch_geometric_data_from_graph(spectr3_G, spectr3_df, spectr_labels_nr, len(spectr_labels_train), len(spectr_labels_valid), len(spectr_labels_test))\n",
    "describe_pyg_data(spectr3_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw files + kNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 413.6495, Acc: 0.1063\n",
      "Epoch: 001, Loss: 2063.6204, Acc: 0.1001\n",
      "Epoch: 002, Loss: 2747.2488, Acc: 0.1036\n",
      "Epoch: 003, Loss: 2429.0723, Acc: 0.0931\n",
      "Epoch: 004, Loss: 1884.4980, Acc: 0.0978\n",
      "Epoch: 005, Loss: 1945.1971, Acc: 0.1048\n",
      "Epoch: 006, Loss: 1791.4187, Acc: 0.1067\n",
      "Early stopping at epoch 6\n"
     ]
    }
   ],
   "source": [
    "GraphSAGE256_model = GraphSAGE(raw_audio_data, 256)\n",
    "train_model(GraphSAGE256_model, raw_audio_data, 100, es_patience=10, es_threshold=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 4.1580, Acc: 0.1414\n",
      "Epoch: 001, Loss: 9.3317, Acc: 0.1289\n",
      "Epoch: 002, Loss: 6.6050, Acc: 0.1554\n",
      "Epoch: 003, Loss: 3.9215, Acc: 0.1578\n",
      "Epoch: 004, Loss: 2.7383, Acc: 0.1741\n",
      "Epoch: 005, Loss: 2.3263, Acc: 0.1488\n",
      "Epoch: 006, Loss: 2.2447, Acc: 0.1480\n",
      "Epoch: 007, Loss: 2.2256, Acc: 0.1434\n",
      "Epoch: 008, Loss: 2.2088, Acc: 0.1414\n",
      "Epoch: 009, Loss: 2.1962, Acc: 0.1422\n",
      "Epoch: 010, Loss: 2.1815, Acc: 0.1508\n",
      "Epoch: 011, Loss: 2.1543, Acc: 0.1574\n",
      "Epoch: 012, Loss: 2.1372, Acc: 0.1671\n",
      "Epoch: 013, Loss: 2.1190, Acc: 0.1808\n",
      "Epoch: 014, Loss: 2.0935, Acc: 0.1839\n",
      "Epoch: 015, Loss: 2.0730, Acc: 0.1995\n",
      "Epoch: 016, Loss: 2.0472, Acc: 0.2287\n",
      "Epoch: 017, Loss: 2.0231, Acc: 0.2392\n",
      "Epoch: 018, Loss: 2.0058, Acc: 0.2333\n",
      "Epoch: 019, Loss: 1.9846, Acc: 0.2411\n",
      "Epoch: 020, Loss: 1.9651, Acc: 0.2489\n",
      "Epoch: 021, Loss: 1.9426, Acc: 0.2497\n",
      "Epoch: 022, Loss: 1.9268, Acc: 0.2450\n",
      "Epoch: 023, Loss: 1.8993, Acc: 0.2485\n",
      "Epoch: 024, Loss: 1.8931, Acc: 0.2579\n",
      "Epoch: 025, Loss: 1.8765, Acc: 0.2633\n",
      "Epoch: 026, Loss: 1.8608, Acc: 0.2692\n",
      "Epoch: 027, Loss: 1.8459, Acc: 0.2637\n",
      "Epoch: 028, Loss: 1.8300, Acc: 0.2649\n",
      "Epoch: 029, Loss: 1.8247, Acc: 0.2672\n",
      "Epoch: 030, Loss: 1.8027, Acc: 0.2743\n",
      "Epoch: 031, Loss: 1.7936, Acc: 0.2711\n",
      "Epoch: 032, Loss: 1.7951, Acc: 0.2723\n",
      "Epoch: 033, Loss: 1.7763, Acc: 0.2727\n",
      "Epoch: 034, Loss: 1.7632, Acc: 0.2766\n",
      "Epoch: 035, Loss: 1.7546, Acc: 0.2817\n",
      "Epoch: 036, Loss: 1.7484, Acc: 0.2774\n",
      "Epoch: 037, Loss: 1.7314, Acc: 0.2797\n",
      "Epoch: 038, Loss: 1.7347, Acc: 0.2817\n",
      "Epoch: 039, Loss: 1.7276, Acc: 0.2805\n",
      "Epoch: 040, Loss: 1.7154, Acc: 0.2891\n",
      "Epoch: 041, Loss: 1.7140, Acc: 0.2906\n",
      "Epoch: 042, Loss: 1.7074, Acc: 0.2891\n",
      "Epoch: 043, Loss: 1.7084, Acc: 0.2906\n",
      "Epoch: 044, Loss: 1.6926, Acc: 0.2933\n",
      "Epoch: 045, Loss: 1.6937, Acc: 0.2933\n",
      "Epoch: 046, Loss: 1.6823, Acc: 0.2972\n",
      "Epoch: 047, Loss: 1.6720, Acc: 0.2984\n",
      "Epoch: 048, Loss: 1.6728, Acc: 0.2996\n",
      "Epoch: 049, Loss: 1.6665, Acc: 0.3031\n",
      "Epoch: 050, Loss: 1.6508, Acc: 0.3105\n",
      "Epoch: 051, Loss: 1.6470, Acc: 0.3144\n",
      "Epoch: 052, Loss: 1.6542, Acc: 0.3140\n",
      "Epoch: 053, Loss: 1.6431, Acc: 0.3198\n",
      "Epoch: 054, Loss: 1.6376, Acc: 0.3175\n",
      "Epoch: 055, Loss: 1.6254, Acc: 0.3163\n",
      "Epoch: 056, Loss: 1.6321, Acc: 0.3175\n",
      "Epoch: 057, Loss: 1.6186, Acc: 0.3210\n",
      "Epoch: 058, Loss: 1.6112, Acc: 0.3202\n",
      "Epoch: 059, Loss: 1.6004, Acc: 0.3249\n",
      "Epoch: 060, Loss: 1.6155, Acc: 0.3268\n",
      "Epoch: 061, Loss: 1.5944, Acc: 0.3296\n",
      "Epoch: 062, Loss: 1.5915, Acc: 0.3284\n",
      "Epoch: 063, Loss: 1.5806, Acc: 0.3311\n",
      "Epoch: 064, Loss: 1.5813, Acc: 0.3307\n",
      "Epoch: 065, Loss: 1.5827, Acc: 0.3284\n",
      "Epoch: 066, Loss: 1.5653, Acc: 0.3335\n",
      "Epoch: 067, Loss: 1.5731, Acc: 0.3276\n",
      "Epoch: 068, Loss: 1.5657, Acc: 0.3268\n",
      "Epoch: 069, Loss: 1.5629, Acc: 0.3335\n",
      "Early stopping at epoch 69\n"
     ]
    }
   ],
   "source": [
    "GraphSAGE256_model = GraphSAGE(spectr2_data, 256)\n",
    "train_model(GraphSAGE256_model, spectr2_data, 100, es_patience=10, es_threshold=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000, Loss: 5.3805, Acc: 0.1250\n",
      "Epoch: 001, Loss: 20.3889, Acc: 0.1145\n",
      "Epoch: 002, Loss: 9.7058, Acc: 0.1660\n",
      "Epoch: 003, Loss: 4.1486, Acc: 0.1457\n",
      "Epoch: 004, Loss: 2.8536, Acc: 0.1449\n",
      "Epoch: 005, Loss: 2.3922, Acc: 0.1434\n",
      "Epoch: 006, Loss: 2.2077, Acc: 0.1247\n",
      "Epoch: 007, Loss: 2.1762, Acc: 0.1375\n",
      "Epoch: 008, Loss: 2.1786, Acc: 0.1426\n",
      "Epoch: 009, Loss: 2.1668, Acc: 0.1414\n",
      "Epoch: 010, Loss: 2.1577, Acc: 0.1387\n",
      "Epoch: 011, Loss: 2.1270, Acc: 0.1445\n",
      "Epoch: 012, Loss: 2.1028, Acc: 0.1531\n",
      "Epoch: 013, Loss: 2.0835, Acc: 0.1539\n",
      "Epoch: 014, Loss: 2.0616, Acc: 0.1617\n",
      "Epoch: 015, Loss: 2.0442, Acc: 0.1621\n",
      "Epoch: 016, Loss: 2.0200, Acc: 0.1757\n",
      "Epoch: 017, Loss: 1.9892, Acc: 0.1924\n",
      "Epoch: 018, Loss: 1.9750, Acc: 0.1979\n",
      "Epoch: 019, Loss: 1.9546, Acc: 0.2022\n",
      "Epoch: 020, Loss: 1.9500, Acc: 0.2069\n",
      "Epoch: 021, Loss: 1.9334, Acc: 0.1959\n",
      "Epoch: 022, Loss: 1.9013, Acc: 0.1995\n",
      "Epoch: 023, Loss: 1.8883, Acc: 0.2174\n",
      "Epoch: 024, Loss: 1.8776, Acc: 0.2170\n",
      "Epoch: 025, Loss: 1.8688, Acc: 0.2174\n",
      "Epoch: 026, Loss: 1.8543, Acc: 0.2220\n",
      "Early stopping at epoch 26\n"
     ]
    }
   ],
   "source": [
    "GraphSAGE256_model = GraphSAGE(spectr3_data, 256)\n",
    "train_model(GraphSAGE256_model, spectr3_data, 100, es_patience=10, es_threshold=0.001)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
