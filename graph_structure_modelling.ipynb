{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5891fb0",
   "metadata": {},
   "source": [
    "# Graph structure modelling \n",
    "\n",
    "- functions permitting to model the 1) graph taxonomy and ensure 2) graph regularizarion\n",
    "- from https://github.com/PlusRoss/GRCN/tree/master "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ef413dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import random\n",
    "import time\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f6f5db",
   "metadata": {},
   "source": [
    "# Building graph from the data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1bd2f17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"train2s.csv\", header = None)\n",
    "data = pd.read_csv(\"test3s.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8158f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>12.6875</td>\n",
       "      <td>25.0625</td>\n",
       "      <td>14.2500</td>\n",
       "      <td>9.3125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.0000</td>\n",
       "      <td>33.1250</td>\n",
       "      <td>32.6250</td>\n",
       "      <td>10.6250</td>\n",
       "      <td>5.1875</td>\n",
       "      <td>6.6875</td>\n",
       "      <td>7.1875</td>\n",
       "      <td>6.5000</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.1250</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>0.0625</td>\n",
       "      <td>4.4375</td>\n",
       "      <td>8.7500</td>\n",
       "      <td>22.3750</td>\n",
       "      <td>68.3750</td>\n",
       "      <td>65.1875</td>\n",
       "      <td>60.5000</td>\n",
       "      <td>34.6250</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>7.6875</td>\n",
       "      <td>24.1875</td>\n",
       "      <td>26.6875</td>\n",
       "      <td>24.4375</td>\n",
       "      <td>56.3125</td>\n",
       "      <td>16.8750</td>\n",
       "      <td>3.3750</td>\n",
       "      <td>4.9375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>18.8125</td>\n",
       "      <td>72.3125</td>\n",
       "      <td>54.6875</td>\n",
       "      <td>1.7500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.8125</td>\n",
       "      <td>26.0625</td>\n",
       "      <td>1.6875</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>6.0000</td>\n",
       "      <td>64.5000</td>\n",
       "      <td>88.5000</td>\n",
       "      <td>14.8125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>3.5000</td>\n",
       "      <td>1.8125</td>\n",
       "      <td>0.8125</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564</th>\n",
       "      <td>0.1250</td>\n",
       "      <td>23.6250</td>\n",
       "      <td>9.0625</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>0.0625</td>\n",
       "      <td>5.3750</td>\n",
       "      <td>60.5000</td>\n",
       "      <td>19.3750</td>\n",
       "      <td>0.6250</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2566</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1.5000</td>\n",
       "      <td>6.4375</td>\n",
       "      <td>9.8125</td>\n",
       "      <td>15.4375</td>\n",
       "      <td>19.6250</td>\n",
       "      <td>20.3125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2500</td>\n",
       "      <td>0.5000</td>\n",
       "      <td>0.6875</td>\n",
       "      <td>0.3125</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2567 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0        1        2        3        4        5        6        7    \\\n",
       "0     0.0000   0.0000   0.0000   0.0000   0.0000   0.0000  12.6875  25.0625   \n",
       "1     3.0000  33.1250  32.6250  10.6250   5.1875   6.6875   7.1875   6.5000   \n",
       "2     0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   \n",
       "3     0.0000   0.0625   0.0625   4.4375   8.7500  22.3750  68.3750  65.1875   \n",
       "4     0.0000   0.0000   7.6875  24.1875  26.6875  24.4375  56.3125  16.8750   \n",
       "...      ...      ...      ...      ...      ...      ...      ...      ...   \n",
       "2562  0.0000   0.0000   0.0000   0.0000   0.0000   0.6875  18.8125  72.3125   \n",
       "2563  0.0000   0.0000   0.0000   0.0000   0.0000   0.0000   6.0000  64.5000   \n",
       "2564  0.1250  23.6250   9.0625   0.3125   0.0000   0.0000   0.0000   0.0000   \n",
       "2565  0.0625   5.3750  60.5000  19.3750   0.6250   0.0000   0.0000   0.0000   \n",
       "2566  0.0000   0.0000   0.0000   0.0000   1.5000   6.4375   9.8125  15.4375   \n",
       "\n",
       "          8        9    ...  246  247     248      249     250     251  \\\n",
       "0     14.2500   9.3125  ...  0.0  0.0  0.0000   0.0000  0.0000  0.0000   \n",
       "1      0.0625   0.0625  ...  0.0  0.0  0.0000   0.0000  0.0000  0.0000   \n",
       "2      0.1250   0.2500  ...  0.0  0.0  0.0000   0.0000  0.0000  0.0000   \n",
       "3     60.5000  34.6250  ...  0.0  0.0  0.0000   0.0000  0.0000  0.0000   \n",
       "4      3.3750   4.9375  ...  0.0  0.0  0.0000   0.0000  0.0000  0.0000   \n",
       "...       ...      ...  ...  ...  ...     ...      ...     ...     ...   \n",
       "2562  54.6875   1.7500  ...  0.0  0.0  8.8125  26.0625  1.6875  0.0000   \n",
       "2563  88.5000  14.8125  ...  0.0  0.0  0.0000   0.2500  3.5000  1.8125   \n",
       "2564   0.0000   0.0000  ...  0.0  0.0  0.0000   0.0000  0.0000  0.0000   \n",
       "2565   0.0000   0.0000  ...  0.0  0.0  0.0000   0.0000  0.0000  0.0000   \n",
       "2566  19.6250  20.3125  ...  0.0  0.0  0.0000   0.0000  0.0000  0.2500   \n",
       "\n",
       "         252     253     254  255  \n",
       "0     0.0000  0.0000  0.0000  0.0  \n",
       "1     0.0000  0.0000  0.0000  0.0  \n",
       "2     0.0000  0.0000  0.0000  0.0  \n",
       "3     0.0000  0.0000  0.0000  0.0  \n",
       "4     0.0000  0.0000  0.0000  0.0  \n",
       "...      ...     ...     ...  ...  \n",
       "2562  0.0000  0.0000  0.0000  0.0  \n",
       "2563  0.8125  0.0000  0.0000  0.0  \n",
       "2564  0.0000  0.0000  0.0000  0.0  \n",
       "2565  0.0000  0.0000  0.0000  0.0  \n",
       "2566  0.5000  0.6875  0.3125  0.0  \n",
       "\n",
       "[2567 rows x 256 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50cdff9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_train  # list of labels\n",
    "\n",
    "\n",
    "# Loading of datasets\n",
    "\n",
    "spectr_train_list = pd.read_csv(\"spectr_train_list.txt\", index_col = 0).iloc[:,0].values.tolist()\n",
    "spectr_test_list = pd.read_csv(\"spectr_test_list.txt\", index_col = 0).iloc[:,0].values.tolist()\n",
    "# spectr_valid_list = pd.read_csv(\"spectr_valid_list.txt\", index_col = 0).iloc[:,0].values.tolist()\n",
    "\n",
    "# labels_train, labels_test\n",
    "# spectrograms_train, spectrograms_test\n",
    "\n",
    "labels_train = []\n",
    "\n",
    "for path in spectr_train_list:\n",
    "    labels_train.append(path.split(\"/\")[1])\n",
    "    \n",
    "labels_test = []\n",
    "\n",
    "for path in spectr_test_list:\n",
    "    labels_test.append(path.split(\"/\")[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed22e97",
   "metadata": {},
   "source": [
    "# BASIC kNN method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8db5e533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacencyMatrixUsingMnearestNeighbors(X, M = 5):\n",
    "    \n",
    "    knn = NearestNeighbors(n_neighbors=M)\n",
    "    knn.fit(X)\n",
    "    #dist_indx_arr = knn.kneighbors(X, return_distance=True, n_neighbors = M)\n",
    "    \n",
    "    A = kneighbors_graph(X, n_neighbors=(M-1), p=2, mode='connectivity', include_self=False) \n",
    "    A = A.toarray() # for some reason, is not symmetric\n",
    "    A = np.maximum( A, A.T )\n",
    "    \n",
    "    return A#, dist_indx_arr # dist_indx_arr will be useful while connecting the graph\n",
    "\n",
    "\n",
    "# Can be used if needed - maybe the graph does not need to be connected?\n",
    "#\n",
    "# Function implements trivial method of connecting the graph\n",
    "#\n",
    "# A - adjacency matrix\n",
    "# returns: adjacency matrix with additional connections\n",
    "def connectTheGraph(A):\n",
    "    # trivial case of adding an edge:\n",
    "    W = A\n",
    "    G = nx.from_numpy_array(W)\n",
    "    graphs = list(nx.connected_components(G))\n",
    "    for i in range(len(graphs)-1):\n",
    "\n",
    "        for g in graphs[i]:\n",
    "            for f in graphs[i+1]:\n",
    "                W[f,g] = 1\n",
    "                W[g,f] = 1\n",
    "                break\n",
    "            break\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a1d47a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "A = adjacencyMatrixUsingMnearestNeighbors(data, M=5) # it needs 1-10s (seems increase linearly with n of observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "302544d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_numpy_array(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4485b7d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nx.is_connected(G)\n",
    "# nx.draw_kamada_kawai(G) # do not draw - it is too computationally demanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b8e358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c46cca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b10f14ab",
   "metadata": {},
   "source": [
    "# GRCN method - todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "963cd23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SGConv, GATConv, knn_graph\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import numpy as np\n",
    "from model_utils import GCNConv_diag, GCNConv_dense, EOS\n",
    "# import torch_sparse as ts # it poses problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1823f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch_sparse in c:\\users\\patry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.6.17)\n",
      "Requirement already satisfied: scipy in c:\\users\\patry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch_sparse) (1.9.3)\n",
      "Requirement already satisfied: numpy<1.26.0,>=1.18.5 in c:\\users\\patry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scipy->torch_sparse) (1.24.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\patry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -iver (c:\\users\\patry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\patry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -iver (c:\\users\\patry\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c536f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, num_nodes, num_features, num_classes, device, args):\n",
    "        super(Model, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_features = num_features\n",
    "        self.graph_nhid = int(args.hid_graph.split(\":\")[0])\n",
    "        self.graph_nhid2 = int(args.hid_graph.split(\":\")[1])\n",
    "        self.nhid = args.nhid\n",
    "        self.conv1 = GCNConv_dense(num_features, self.nhid)\n",
    "        self.conv2 = GCNConv_dense(self.nhid, num_classes)\n",
    "        if args.layertype == \"dense\":\n",
    "            self.conv_graph = GCNConv_dense(num_features, self.graph_nhid)\n",
    "            self.conv_graph2 = GCNConv_dense(self.graph_nhid, self.graph_nhid2)\n",
    "        elif args.layertype == \"diag\":\n",
    "            self.conv_graph = GCNConv_diag(num_features, device)\n",
    "            self.conv_graph2 = GCNConv_diag(num_features, device)\n",
    "        else:\n",
    "            exit(\"wrong layer type\")\n",
    "        self.F = args.F\n",
    "        self.F_graph = args.F_graph\n",
    "        self.dropout = args.dropout\n",
    "        self.K = args.compl_param.split(\":\")[0]\n",
    "        self.mask = None\n",
    "        self.Adj_new = None\n",
    "        self._normalize = args.normalize\n",
    "        self.device = device\n",
    "        self.reduce = args.reduce\n",
    "        self.sparse = args.sparse\n",
    "        self.norm_mode = \"sym\"\n",
    "        self.topindex = None\n",
    "\n",
    "    def init_para(self):\n",
    "        self.conv1.init_para()\n",
    "        self.conv2.init_para()\n",
    "        self.conv_graph.init_para()\n",
    "        self.conv_graph2.init_para()\n",
    "\n",
    "    def graph_parameters(self):\n",
    "        return list(self.conv_graph.parameters()) + list(self.conv_graph2.parameters())\n",
    "\n",
    "    def base_parameters(self):\n",
    "        return list(self.conv1.parameters()) + list(self.conv2.parameters())\n",
    "\n",
    "    def cal_similarity_graph(self, node_embeddings):\n",
    "        # similarity_graph = torch.mm(node_embeddings, node_embeddings.t())\n",
    "        similarity_graph = torch.mm(node_embeddings[:, :int(self.num_features/2)], node_embeddings[:, :int(self.num_features/2)].t())\n",
    "        similarity_graph += torch.mm(node_embeddings[:, int(self.num_features/2):], node_embeddings[:, int(self.num_features/2):].t())\n",
    "        # node_embeddings_top = node_embeddings[self.topindex]\n",
    "        # similarity_graph = node_embeddings.unsqueeze(1) * node_embeddings_top\n",
    "        # similarity_graph = torch.sum(similarity_graph, dim=-1)\n",
    "        return similarity_graph\n",
    "\n",
    "    def normalize(self, adj, mode=\"sym\" ,sparse=False):\n",
    "        if not sparse:\n",
    "            if mode == \"sym\":\n",
    "                inv_sqrt_degree = 1. / (torch.sqrt(adj.sum(dim=1, keepdim=False)) + EOS)\n",
    "                return inv_sqrt_degree[:, None] * adj * inv_sqrt_degree[None, :]\n",
    "            elif mode == \"row\":\n",
    "                inv_degree = 1. / (adj.sum(dim=1, keepdim=False) + EOS)\n",
    "                return inv_degree[:, None] * adj\n",
    "            else:\n",
    "                exit(\"wrong norm mode\")\n",
    "        else:\n",
    "            adj = adj.coalesce()\n",
    "            if mode == \"sym\":\n",
    "                inv_sqrt_degree = 1. / (torch.sqrt(torch.sparse.sum(adj, dim=1).values()) + EOS)\n",
    "                D_value = inv_sqrt_degree[adj.indices()[0]] * inv_sqrt_degree[adj.indices()[1]]\n",
    "            elif mode == \"row\":\n",
    "                inv_degree = 1. / (torch.sparse.sum(adj, dim=1).values() + EOS)\n",
    "                D_value = inv_degree[adj.indices()[0]]\n",
    "            else:\n",
    "                exit(\"wrong norm mode\")\n",
    "            new_values = adj.values() * D_value\n",
    "            return torch.sparse.FloatTensor(adj.indices(), new_values, adj.size()).to(self.device)\n",
    "\n",
    "    def _sparse_graph(self, raw_graph, K, sparse):\n",
    "        if self.reduce == \"knn\":\n",
    "            if self.topindex is None:\n",
    "                values, indices = raw_graph.topk(k=int(K), dim=-1)\n",
    "                self.topindex = indices\n",
    "                assert torch.max(indices) < raw_graph.shape[1]\n",
    "            else:\n",
    "                indices = self.topindex\n",
    "                values = raw_graph[torch.arange(raw_graph.shape[0]).view(-1,1), indices]\n",
    "            assert torch.sum(torch.isnan(values)) == 0\n",
    "            if not sparse:\n",
    "                self.mask = torch.zeros(raw_graph.shape).to(self.device)\n",
    "                self.mask[torch.arange(raw_graph.shape[0]).view(-1,1), indices] = 1.\n",
    "                self.mask[indices, torch.arange(raw_graph.shape[1]).view(-1,1)] = 1.\n",
    "            else:\n",
    "                inds = torch.stack([torch.arange(raw_graph.shape[0]).view(-1,1).expand(-1,int(K)).contiguous().view(1,-1)[0].to(self.device),\n",
    "                                     indices.view(1,-1)[0]])\n",
    "                inds = torch.cat([inds, torch.stack([inds[1], inds[0]])], dim=1)\n",
    "                values = torch.cat([values.view(1,-1)[0], values.view(1,-1)[0]])\n",
    "                return inds, values\n",
    "        else:\n",
    "            exit(\"wrong sparsification method\")\n",
    "        self.mask.requires_grad = False\n",
    "        sparse_graph = raw_graph * self.mask\n",
    "        return sparse_graph\n",
    "\n",
    "    def _node_embeddings(self, input, Adj, sparse=False):\n",
    "        norm_Adj = self.normalize(Adj, self.norm_mode, sparse)\n",
    "        if self.F_graph != \"identity\":\n",
    "            node_embeddings = self.F_graph(self.conv_graph(input, norm_Adj, sparse))\n",
    "            node_embeddings = self.conv_graph2(node_embeddings, norm_Adj, sparse)\n",
    "        else:\n",
    "            node_embeddings = self.conv_graph(input, norm_Adj, sparse)\n",
    "            node_embeddings = self.conv_graph2(node_embeddings, norm_Adj, sparse)\n",
    "        if self._normalize:\n",
    "            node_embeddings = F.normalize(node_embeddings, dim=1, p=2)\n",
    "        return node_embeddings\n",
    "\n",
    "    def forward(self, input, Adj):\n",
    "        Adj.requires_grad = False\n",
    "        node_embeddings = self._node_embeddings(input, Adj, self.sparse)\n",
    "        Adj_new = self.cal_similarity_graph(node_embeddings)\n",
    "\n",
    "        if not self.sparse:\n",
    "            Adj_new = self._sparse_graph(Adj_new, self.K, self.sparse)\n",
    "            Adj_new = self.normalize(Adj + Adj_new, self.norm_mode)\n",
    "        else:\n",
    "            Adj_new_indices, Adj_new_values = self._sparse_graph(Adj_new, self.K, self.sparse)\n",
    "            new_inds = torch.cat([Adj.indices(), Adj_new_indices], dim=1)\n",
    "            new_values = torch.cat([Adj.values(), Adj_new_values])\n",
    "            Adj_new = torch.sparse.FloatTensor(new_inds, new_values, Adj.size()).to(self.device)\n",
    "            Adj_new = self.normalize(Adj_new, self.norm_mode, self.sparse)\n",
    "\n",
    "        x = self.conv1(input, Adj_new, self.sparse)\n",
    "        x = F.dropout(self.F(x), training=self.training, p=self.dropout)\n",
    "        x = self.conv2(x, Adj_new, self.sparse)\n",
    "\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ec4db4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abac7792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81190e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac463cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
